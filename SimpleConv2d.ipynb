{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleConv2d.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPJFXHJ+q12CFqGJvzWm6wG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baroka-wp/dive_ML/blob/master/SimpleConv2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "BQrvsYV_j58f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "from keras.datasets import mnist\n",
        "from keras.callbacks import TensorBoard\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Préparation du jeu de données\n"
      ],
      "metadata": {
        "id": "WJC4GkEBkEch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# chargement des donnêes\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#visualisation\n",
        "index = 0\n",
        "image = X_train[index].reshape(28,28)\n",
        "plt.imshow(image, 'gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "8fZkSqZZkIP1",
        "outputId": "50e3a7ce-2bf4-450b-bb05-ece7844e4c7f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#traiment et normalisation des données\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "X_train = X_train[:, np.newaxis, :, :]\n",
        "X_test = X_test[:, np.newaxis, :, :]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "print(X_train.shape, y_train.shape) # (48000, 784)\n",
        "print(X_val.shape, y_val.shape) # (12000, 784)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VinUKk6ckiha",
        "outputId": "5b4596d9-c1a9-4717-b33c-5d70f09d141f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 1, 28, 28) (48000, 10)\n",
            "(12000, 1, 28, 28) (12000, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 1] Création d'une couche de convolution bidimensionnelle\n"
      ],
      "metadata": {
        "id": "FxZDZynVkXTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleConv2d():\n",
        "    def __init__(self, F, C, FH, FW, P, S,initializer=None,optimizer=None,activation=None):\n",
        "        self.P = P\n",
        "        self.S = S\n",
        "        self.initializer = initializer\n",
        "        self.optimizer = optimizer\n",
        "        self.activation = activation\n",
        "        self.W = self.initializer.W(F,C,FH,FW)\n",
        "        self.B = self.initializer.B(F)\n",
        "    \n",
        "\n",
        "    def forward(self, X,debug=False):\n",
        "      self.X = X\n",
        "      N,C,H,W = self.X.shape\n",
        "      F,C,FH,FW = self.W.shape\n",
        "      OH,OW = self.output_shape2d(H,W,self.P,self.P,FH,FW,self.S,self.S)\n",
        "      self.params = N,C,H,W,F,FH,FW,OH,OW\n",
        "      A = np.zeros([N,F,OH,OW])\n",
        "      self.X_pad = np.pad(self.X,((0,0),(0,0),(self.P,self.P),(self.P,self.P)))\n",
        "      for n in range(N):\n",
        "          for ch in range(F):\n",
        "              for row in range(0,H,self.S):\n",
        "                  for col in range(0,W,self.S):\n",
        "                      if self.P == 0 and (W-2 <= col or H-2<=row):\n",
        "                          continue\n",
        "                      A[n,ch,row,col] = np.sum(self.X_pad[n,:,row:row+FH,col:col+FW]*self.W[ch,:,:,:]) +self.B[ch]\n",
        "      if debug==True:\n",
        "          return A\n",
        "      else:\n",
        "          return  self.activation.forward(A)\n",
        "\n",
        "    def backward(self, dZ,debug=False):\n",
        "      if debug==True:\n",
        "          dA = dZ\n",
        "      else:\n",
        "          dA = self.activation.backward(dZ)\n",
        "      N,C,H,W,F,FH,FW,OH,OW = self.params\n",
        "      dZ = np.zeros(self.X_pad.shape)\n",
        "      self.dW = np.zeros(self.W.shape)\n",
        "      self.dB = np.zeros(self.B.shape)\n",
        "      for n in range(N):\n",
        "          for ch in range(F):\n",
        "              for row in range(0,H,self.S):\n",
        "                  for col in range(0,W,self.S):\n",
        "                      if self.P == 0 and (W-2 <= col or H-2<=row):\n",
        "                          continue\n",
        "                      dZ[n,:,row:row+FH,col:col+FW] += dA[n,ch,row,col]*self.W[ch,:,:,:]\n",
        "      if self.P == 0:\n",
        "          dZ = np.delete(dZ,[0,H-1],axis=2)\n",
        "          dZ = np.delete(dZ,[0,W-1],axis=3)\n",
        "      else:\n",
        "          dl_rows = range(self.P),range(H+self.P,H+2*self.P,1)\n",
        "          dl_cols = range(self.P),range(W+self.P,W+2*self.P,1)\n",
        "          dZ = np.delete(dZ,dl_rows,axis=2)\n",
        "          dZ = np.delete(dZ,dl_cols,axis=3)\n",
        "      for n in range(N):\n",
        "          for ch in range(F):\n",
        "              for row in range(OH):\n",
        "                  for col in range(OW):\n",
        "                      self.dW[ch,:,:,:] += dA[n,ch,row,col]*self.X_pad[n,:,row:row+FH,col:col+FW]\n",
        "      for ch in range(F):\n",
        "          self.dB[ch] = np.sum(dA[:,ch,:,:])\n",
        "      self = self.optimizer.update(self)\n",
        "      return dZ\n",
        "\n",
        "    def output_shape2d(self,H,W,PH,PW,FH,FW,SH,SW):\n",
        "      OH = (H +2*PH -FH)/SH +1\n",
        "      OW = (W +2*PW -FW)/SW +1\n",
        "      return int(OH),int(OW)"
      ],
      "metadata": {
        "id": "o1MV_xDikYj5"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class d'initialisation"
      ],
      "metadata": {
        "id": "U5s-y2XlcGK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleInitializerConv2d:\n",
        "    def __init__(self, sigma=0.01):\n",
        "        self.sigma = sigma\n",
        "    def W(self, F, C, FH, FW):\n",
        "        return self.sigma * np.random.randn(F,C,FH,FW)\n",
        "    def B(self, F):\n",
        "        return np.zeros(F)"
      ],
      "metadata": {
        "id": "FggBwg-XcrMb"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class d'activation"
      ],
      "metadata": {
        "id": "qgT2AOUpciSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "        \n",
        "class ReLU:\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)"
      ],
      "metadata": {
        "id": "REm-a2kdcF1d"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Class d'Optimizer"
      ],
      "metadata": {
        "id": "tokegaHQcbKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return"
      ],
      "metadata": {
        "id": "dkJ3THg-ceNd"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 2] Expérience d'une couche de convolution bidimensionnelle avec un petit tableau\n"
      ],
      "metadata": {
        "id": "FuNCjuukbjjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[[[ 1,  2,  3,  4],[ 5,  6,  7,  8],[ 9, 10, 11, 12],[13, 14, 15, 16]]]])\n",
        "w = np.array([[[[ 0.,  0.,  0.],[ 0.,  1.,  0.],[ 0., -1.,  0.]]],[[[ 0.,  0.,  0.],[ 0., -1.,  1.],[ 0.,  0.,  0.]]]])"
      ],
      "metadata": {
        "id": "elVq9vqzbXdW"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_conv_2d = SimpleConv2d(F=2, C=1, FH=3, FW=3, P=0, S=1,initializer=SimpleInitializerConv2d(),optimizer=SGD(1000),activation=ReLU())\n",
        "simple_conv_2d.W = w\n",
        "A = simple_conv_2d.forward(x,True)\n",
        "print(A)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwxV_HKEbOYn",
        "outputId": "06ba4cb7-8002-443f-fb2c-7e6ea975240b"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-4. -4.]\n",
            "   [-4. -4.]]\n",
            "\n",
            "  [[ 1.  1.]\n",
            "   [ 1.  1.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delta_a = np.array([[[[ -4,  -4], [ 10,  11]],[[  1,  -7],[  1, -11]]]])\n",
        "dZ = simple_conv_2d.backward(delta_a,True)\n",
        "print(dZ)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEz_PumufU_z",
        "outputId": "3aa84923-9245-4489-96a6-2c150a5689c6"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[-5.  4.]\n",
            "   [13. 27.]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 3] Taille de sortie après convolution 2D\n"
      ],
      "metadata": {
        "id": "MdAxEQpgfd9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_conv_2d.output_shape2d(H=6,W=6,PH=0,PW=0,FH=3,FW=3,SH=1,SW=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Cn3GV4ffcf",
        "outputId": "c1e4e21a-1e43-4cc8-ba88-132772509673"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 4] Création d'une couche de mutualisation maximale\n"
      ],
      "metadata": {
        "id": "iiDfmQB7fodP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPool2D:\n",
        "    def __init__(self, pool_h=3, pool_w=3, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "        \n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def _im2col(self, input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "        \"\"\"\n",
        "       Parameters\n",
        "         ----------\n",
        "         input_data: Input data consisting of a 4-dimensional array of (number of data, channels, height, width)\n",
        "         filter_h: Filter height\n",
        "         filter_w: Filter width\n",
        "         stride: stride\n",
        "         pad: padding\n",
        "         Returns\n",
        "         -------\n",
        "         col: 2D array\n",
        "        \"\"\"\n",
        "        N, C, H, W = input_data.shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "        img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "        col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "        col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "        \n",
        "        return col\n",
        "        \n",
        "    def _col2im(self, col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "        N, C, H, W = input_shape\n",
        "        out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "        out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "        col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
        "\n",
        "        img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
        "        for y in range(filter_h):\n",
        "            y_max = y + stride*out_h\n",
        "            for x in range(filter_w):\n",
        "                x_max = x + stride*out_w\n",
        "                img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
        "\n",
        "        return img[:, :, pad:H + pad, pad:W + pad]\n",
        "                \n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = self._im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = np.argmax(col, axis=1)\n",
        "        out = np.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "        \n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = np.zeros((dout.size, pool_size))\n",
        "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
        "        \n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = self._col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        \n",
        "        return dx"
      ],
      "metadata": {
        "id": "LM5oyA1YfpiE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 5] (Tâche avancée) Créer un pooling moyen\n"
      ],
      "metadata": {
        "id": "-HI8fDjigVJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Convolutional(nn.Module):\n",
        "    def __init__(self, input_channels=3, num_classes=10):\n",
        "        super(Convolutional, self).__init__()\n",
        "        self.input_channels = input_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.layer1 = nn.Sequential()\n",
        "        self.layer1.add_module(\"Conv1\", nn.Conv2d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1))\n",
        "        self.layer1.add_module(\"BN1\", nn.BatchNorm2d(num_features=16, eps=1e-05, momentum=0.1, affine=True,\n",
        "            track_running_stats=True))\n",
        "        self.layer1.add_module(\"Relu1\", nn.ReLU(inplace=False))\n",
        "        self.layer2 = nn.Sequential()\n",
        "        self.layer2.add_module(\"Conv2\", nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2))\n",
        "        self.layer2.add_module(\"BN2\", nn.BatchNorm2d(num_features=32, eps=1e-05, momentum=0.1, affine=True,\n",
        "            track_running_stats=True))\n",
        "        self.layer2.add_module(\"Relu2\", nn.ReLU(inplace=False))\n",
        "        self.avg_pool(\"AvgPool1\", nn.AvgPool2D(kernel_size=4, stride=4, padding=0, ceil_mode=False,\n",
        "            count_include_pad=False))\n",
        "        self.fully_connected = nn.Linear(32 * 4 * 4, num_classes)\n",
        "    def forward(self, x):\n",
        "        y = x.clone()\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(-1, 32 * 4 * 4)\n",
        "        x = self.fully_connected(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "eQQg_V08f4zV"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 6] Lissage\n"
      ],
      "metadata": {
        "id": "lOxNlWNHgqxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self,X):\n",
        "        self.shape = X.shape\n",
        "        return X.reshape(len(X),-1)\n",
        "    def backward(self,X):\n",
        "        return X.reshape(self.shape)        "
      ],
      "metadata": {
        "id": "Wx-Y80WkgsZc"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 7] Apprentissage et estimation\n"
      ],
      "metadata": {
        "id": "pbwjk3RqhP7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "    \n",
        "    def forward(self, X):     \n",
        "        self.X = X\n",
        "        return np.dot(self.X, self.W) + self.B\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        self.dB = dA\n",
        "        self.dW = np.dot(self.X.T, dA)\n",
        "        dZ = np.dot(dA, self.W.T)\n",
        "\n",
        "        self = self.optimizer.update(self)\n",
        "        return dZ"
      ],
      "metadata": {
        "id": "seghJXots96X"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class XavierInitializer:\n",
        "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
        "        self.filter_num = filter_num\n",
        "        self.input_channel = input_channel\n",
        "        self.filter_size = filter_size \n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):    \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None:\n",
        "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size)\n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)       \n",
        "        \n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):     \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None:\n",
        "            B = np.random.randn(self.filter_num)\n",
        "        else:\n",
        "            B = np.zeros(n_nodes2)\n",
        "        \n",
        "        return B"
      ],
      "metadata": {
        "id": "NvKFGrYmtkHk"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeInitializer:\n",
        "    def __init__(self, filter_num=None, input_channel=None, filter_size=None):        \n",
        "        self.filter_num = filter_num\n",
        "        self.input_channel = input_channel\n",
        "        self.filter_size = filter_size       \n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2): \n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None:\n",
        "            W = np.random.randn(self.filter_num, self.input_channel, self.filter_size, self.filter_size) * np.sqrt(2 / self.filter_num) \n",
        "        else:\n",
        "            W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
        "    \n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        if self.filter_num and self.input_channel and self.filter_size is not None:\n",
        "            B = np.random.randn(self.filter_num)\n",
        "        else:\n",
        "            B = np.random.randn(n_nodes2)\n",
        "        \n",
        "        return B"
      ],
      "metadata": {
        "id": "ZX35F93xtsTY"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "metadata": {
        "id": "TF-fioyUsMEC"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, layer):\n",
        "        layer.W = layer.W - self.lr * layer.dW\n",
        "        layer.B = layer.B - self.lr * layer.dB.mean(axis=0)\n",
        "        \n",
        "        return layer\n",
        "\n",
        "class AdaGrad:  \n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW= 0\n",
        "        self.HB = 0     \n",
        "\n",
        "    def update(self, layer):\n",
        "        self.HW = np.zeros_like(layer.W)\n",
        "        self.HB = np.zeros_like(layer.B)\n",
        "        self.HW = self.HW + (layer.dW**2)\n",
        "        self.HB = self.HB + (layer.dB**2).mean(axis=0)\n",
        "        layer.W = layer.W - self.lr * 1 / np.sqrt(self.HW + 1e-7) * layer.dW\n",
        "        layer.B = layer.B - self.lr * 1 / np.sqrt(self.HB + 1e-7) * layer.dB.mean(axis=0)\n",
        "        \n",
        "        return layer\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        self.Z = 1.0 / (1.0 + np.exp(-self.A))\n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - self.Z) * self.Z\n",
        "\n",
        "class Tanh:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(self.A)\n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1.0 - (np.tanh(self.A) ** 2))\n",
        "\n",
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, A):    \n",
        "        self.A = A\n",
        "      \n",
        "        return np.maximum(self.A, 0)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "            \n",
        "        return np.where(self.A > 0, dZ, 0)\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, A):\n",
        "        return np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
        "    def backward(self, Z, y):\n",
        "        dA = Z - y\n",
        "        batch_size = y.shape[0]\n",
        "        loss = -np.sum(y * np.log(Z)) / batch_size\n",
        "\n",
        "        return dA, loss"
      ],
      "metadata": {
        "id": "-isJZpI9t-7s"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Scratch2dCNNClassifier:  \n",
        "    def __init__(self, activation, n_nodes, n_output, lr, optimizer, filter_num, filter_size):\n",
        "        self.select_activation = activation\n",
        "        self.n_nodes = n_nodes\n",
        "        self.n_output = n_output\n",
        "        self.lr = lr\n",
        "        self.select_optimizer = optimizer\n",
        "        \n",
        "        self.filter_num  = filter_num    \n",
        "        self.filter_size   = filter_size         \n",
        "        self.stride          = 1               \n",
        "        self.pad             = 0              \n",
        "            \n",
        "    def _initialize_n_layers(self):\n",
        "        self.activation = dict()\n",
        "        self.FC = dict()\n",
        "        if self.select_activation == 'sigmoid':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = Sigmoid()\n",
        "        elif self.select_activation == 'tanh':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = Tanh()\n",
        "        elif self.select_activation == 'relu':\n",
        "            self.FC[0] = FC(self.out_size, self.n_nodes[0], \n",
        "                            HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "            self.activation[0] = ReLU()\n",
        "\n",
        "        for n_layer in range(len(self.n_nodes)):            \n",
        "            if n_layer == len(self.n_nodes) -1:\n",
        "                if self.select_activation == 'sigmoid':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output, \n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "                elif self.select_activation == 'tanh':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "                elif self.select_activation == 'relu':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_output,\n",
        "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Softmax()\n",
        "            else:\n",
        "                if self.select_activation == 'sigmoid':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1], \n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Sigmoid()\n",
        "                elif self.select_activation == 'tanh':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
        "                                              XavierInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = Tanh()\n",
        "                elif self.select_activation == 'relu':\n",
        "                    self.FC[n_layer + 1] = FC(self.n_nodes[n_layer], self.n_nodes[n_layer+1],\n",
        "                                              HeInitializer(filter_num=None, input_channel=None, filter_size=None), self.optimizer)\n",
        "                    self.activation[n_layer + 1] = ReLU()\n",
        "                    \n",
        "    def _calc_out_shape(self, H, FH, W, FW, layer):    \n",
        "        if layer == 'conv':\n",
        "            out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "            out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
        "        elif layer == 'pool':\n",
        "            out_h = int(1 + (H - FH) / self.stride)\n",
        "            out_w = int(1 + (W - FW) / self.stride)\n",
        "        \n",
        "        return out_h, out_w    \n",
        "    \n",
        "    def fit(self, X, y, epochs=10, batch_size=20):  \n",
        "        self.epochs = epochs   \n",
        "        self.batch_size = batch_size\n",
        "        self.loss = np.zeros(self.epochs)\n",
        "        self.loss_val = np.zeros(self.epochs)      \n",
        "        \n",
        "        if self.select_optimizer == 'sgd':\n",
        "            self.optimizer = SGD(self.lr)\n",
        "        elif self.select_optimizer == 'adagrad':\n",
        "            self.optimizer = AdaGrad(self.lr)            \n",
        "        \n",
        "        self.input_channel = X.shape[1]\n",
        "        self.input_h = X.shape[2]\n",
        "        self.input_w = X.shape[3]\n",
        "        \n",
        "        self.conv = Conv2d(self.select_activation, self.optimizer, self.filter_num, self.input_channel, self.filter_size)\n",
        "        \n",
        "        if self.select_activation == 'sigmoid':\n",
        "            self.activation_conv = Sigmoid()\n",
        "        elif self.select_activation == 'tanh':\n",
        "            self.activation_conv = Tanh()\n",
        "        elif self.select_activation == 'relu':\n",
        "            self.activation_conv = ReLU()\n",
        "            \n",
        "        self.pool = MaxPool2D()   \n",
        "        self.flatten = Flatten()\n",
        "        out_h, out_w = self._calc_out_shape(self.input_h, self.filter_size, self.input_w, self.filter_size, 'conv')\n",
        "        out_h, out_w = self._calc_out_shape(out_h, self.filter_size, out_w, self.filter_size, 'pool')\n",
        "        self.out_size = out_h * out_w * self.filter_num\n",
        "        \n",
        "        self._initialize_n_layers()\n",
        "        \n",
        "        get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            for mini_X_train,  mini_y_train in get_mini_batch:\n",
        "                self.X_ = mini_X_train\n",
        "                self.y_ = mini_y_train\n",
        "                \n",
        "                self.A = self.conv.forward(self.X_)\n",
        "                self.Z = self.activation_conv.forward(self.A)\n",
        "                self.P = self.pool.forward(self.Z)\n",
        "                self.F = self.flatten.forward(self.P)\n",
        "                \n",
        "                self.A = self.FC[0].forward(self.F)\n",
        "                self.Z = self.activation[0].forward(self.A)       \n",
        "                for n_layer in range(1, len(self.n_nodes) + 1):\n",
        "                    self.A = self.FC[n_layer].forward(self.Z)\n",
        "                    self.Z = self.activation[n_layer].forward(self.A)\n",
        "                \n",
        "                self.dA, self.loss[epoch] = self.activation[len(self.n_nodes)].backward(self.Z, self.y_)\n",
        "                self.dZ = self.FC[len(self.n_nodes)].backward(self.dA) \n",
        "                for n_layer in reversed(range(0, len(self.n_nodes))): \n",
        "                    self.dA = self.activation[n_layer].backward(self.dZ)\n",
        "                    self.dZ = self.FC[n_layer].backward(self.dA)\n",
        "\n",
        "                self.dF = self.flatten.backward(self.dZ)\n",
        "                self.dP = self.pool.backward(self.dF)\n",
        "                self.dA = self.activation_conv.backward(self.dP)\n",
        "                self.dZ = self.conv.backward(self.dA)\n",
        "                \n",
        "    def predict(self,X):\n",
        "        self.A = self.conv.forward(X)\n",
        "        self.Z = self.activation_conv.forward(self.A)\n",
        "        self.P = self.pool.forward(self.Z)\n",
        "        self.F = self.flatten.forward(self.P)\n",
        "        self.A = self.FC[0].forward(self.F)\n",
        "        self.Z = self.activation[0].forward(self.A)    \n",
        "        for n_layer in range(1, len(self.n_nodes) + 1):\n",
        "            self.A = self.FC[n_layer].forward(self.Z)\n",
        "            self.Z = self.activation[n_layer].forward(self.A)\n",
        "        \n",
        "        return np.argmax(self.Z, axis=1)"
      ],
      "metadata": {
        "id": "-4r3SRrasQDB"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Scratch2dCNNClassifier(activation='relu', n_nodes=[400, 200, 100], n_output=10, lr=0.001, optimizer='sgd', filter_num=3, filter_size=3)\n",
        "model.fit(X_train, y_train,epochs=10, batch_size=20)\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVyfPNtQurBF",
        "outputId": "29575479-e871-473a-c347-740426b201ce"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy : {}\".format(accuracy_score(y_test, y_pred)))\n",
        "print(\"\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWRuaYTxxeP7",
        "outputId": "f468a52e-3e36-489e-c346-397b64960bd4"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy : 0.9641\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       980\n",
            "           1       0.99      0.99      0.99      1135\n",
            "           2       0.96      0.98      0.97      1032\n",
            "           3       0.90      0.99      0.94      1010\n",
            "           4       1.00      0.93      0.96       982\n",
            "           5       0.98      0.92      0.95       892\n",
            "           6       0.98      0.98      0.98       958\n",
            "           7       0.98      0.96      0.97      1028\n",
            "           8       0.94      0.97      0.95       974\n",
            "           9       0.95      0.95      0.95      1009\n",
            "\n",
            "    accuracy                           0.96     10000\n",
            "   macro avg       0.96      0.96      0.96     10000\n",
            "weighted avg       0.97      0.96      0.96     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 8] (tâche avancée) LeNet"
      ],
      "metadata": {
        "id": "Kj5LEc-xvdyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lenet(input_shape, num_classes):\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(6, kernel_size=5, strides=(1, 1), input_shape=input_shape, activation=\"relu\"))\n",
        "  model.add(Conv2D(16, kernel_size=5, strides=(1, 1), activation=\"relu\"))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(120, activation=\"relu\") )\n",
        "  model.add(Dense(84, activation=\"relu\"))\n",
        "  model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "  return model"
      ],
      "metadata": {
        "id": "6p-oZic2vhPN"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset():\n",
        "  def __init__(self):\n",
        "    self.image_shape = (28, 28, 1)\n",
        "    self.num_classes = 10\n",
        "\n",
        "  def get_batch(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    X_train, X_test = [self.preprocess(d) for d in [X_train, X_test]]\n",
        "    y_train, y_test = [self.preprocess(d, label_data=True) for d in [y_train, y_test]]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def preprocess(self, data, label_data=False):\n",
        "    if label_data:\n",
        "      data = keras.utils.to_categorical(data, self.num_classes)\n",
        "    else:\n",
        "      data = data.astype(\"float32\") / 255\n",
        "      shape = (data.shape[0],) + self.image_shape\n",
        "      data = data.reshape(shape)\n",
        "    return data\n",
        "\n",
        "class Trainer():\n",
        "  def __init__(self, model, loss, optimizer, logdir=\"logdir\"):\n",
        "    self.target = model\n",
        "    self.target.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    self.verbose = 1\n",
        "    self.log_dir = os.path.join(os.path.dirname(__file__), logdir)\n",
        "    if not os.path.exists(self.log_dir):\n",
        "      os.mkdir(self.log_dir)\n",
        "\n",
        "  def train(self, X_train, y_train, batch_size, epochs, validation_split):\n",
        "    self.target.fit(X_train, y_train, \n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=validation_split,\n",
        "                    callbacks=[TensorBoard(log_dir=self.log_dir)],\n",
        "                    verbose=self.verbose)"
      ],
      "metadata": {
        "id": "4CXCcA53wJzg"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    __file__\n",
        "except NameError:\n",
        "    __file__ = os.path.join(os.getcwd(),\"dummy\")\n",
        "else:\n",
        "    pass"
      ],
      "metadata": {
        "id": "05naSQJewUi4"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logdir32\n",
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "2TbGCKL7wa29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les CNN, qui constituent un type de réseaux profonds, présentent les avantages suivants par rapport aux modèles à structure superficielle : (1) Les CNN appliquent directement une opération de convolution aux pixels d'une image pour extraire des caractéristiques de données abstraites. Cette extraction de caractéristiques peut être appliquée à divers scénarios et possède une capacité de généralisation plus puissante. (2) Les CNN sont capables de représenter les informations d'image de manière distribuée et d'acquérir rapidement des informations d'image à partir de volumes massifs de données. La structure des CNN peut résoudre efficacement des problèmes non linéaires complexes (par exemple, la rotation et la translation d'une image). (3) Les CNN sont caractérisés par des connexions éparses, un partage des poids et un sous-échantillonnage spatial, ce qui donne une structure de réseau plus simple et plus adaptable aux structures des images. Afin de mieux comprendre la classification d'images basée sur les CNN, cette section présente brièvement la structure des CNN et sa méthode de formation, puis plusieurs modèles CNN populaires dans le domaine de la vision par ordinateur.\n",
        "\n",
        "Calculez la taille de la sortie et le nombre de paramètres pour les trois couches convolutives suivantes. Pour le nombre de paramètres, considérez également le terme de biais.\n",
        "\n",
        "- taille d'entrée : 144×144, 3 canaux taille du filtre : 3×3, 6 canaux stride : 1 padding : aucun\n",
        "\n",
        "- taille d'entrée : 60 x 60, 24 canaux taille du filtre : 3 x 3, 48 canaux stride : 1 padding : none\n",
        "\n",
        "- taille d'entrée : 20x20, 10 canaux taille du filtre : 3x3, 20 canaux stride : 2 padding : none"
      ],
      "metadata": {
        "id": "agW7ow11wi-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 10] Calcul de la taille de sortie et du nombre de paramètres\n"
      ],
      "metadata": {
        "id": "H3ZnLQfox8wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN):\n",
        "    out_h = int(1 + (H + 2*pad - FH) / stride)\n",
        "    out_w = int(1 + (W + 2*pad - FW) / stride)\n",
        "    params = FH * FW * IC * FN + FN ## Finally add the bias term + FN\n",
        "    activation_size = FN * out_h * out_w\n",
        "    return out_h, out_w, params, activation_size\n",
        "\n",
        "\n",
        "##Input size\n",
        "H, W, IC = (144, 144, 3) # IC = input_channel\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 3) ## FC (unused)= filter_channel\n",
        "##Number of filters (The filter size of 6 channels specified in the problem is assumed to be the number of filters)\n",
        "FN = 6\n",
        "##Stride, padding\n",
        "stride, pad = 1, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74ayhnEVwnSB",
        "outputId": "b2f0e0d5-6bc7-4659-c6b0-d3e89f752e95"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 6 × 142 × 142\n",
            "Activation Size : 120984\n",
            "Number of parameters : 168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Input size\n",
        "H, W, IC = (60, 60, 24) ## IC = input_channel\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 24) ## FC(unused) = filter_channel\n",
        "##Number of filters (The problem-specified filter size of 48 channels is assumed to be the number of filters)\n",
        "FN = 48\n",
        "##Stride, padding\n",
        "stride, pad = 1, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg4jfrLsyZWp",
        "outputId": "b7ca6396-125e-453d-84a2-e67d56a4221b"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 48 × 58 × 58\n",
            "Activation Size : 161472\n",
            "Number of parameters : 10416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Input size\n",
        "H, W, IC = (20, 20, 10) ## IC = input_channel\n",
        "\n",
        "##Filter size\n",
        "FH, FW, FC = (3, 3, 10) ## FC(unused) = filter_channel\n",
        "\n",
        "##Number of filters (The 20-channel filter size specified in the problem is assumed to be the number of filters)\n",
        "FN = 20\n",
        "\n",
        "##Stride, padding\n",
        "stride, pad = 2, 0\n",
        "\n",
        "out_h, out_w, params, activation_size = calc_out_shape_and_params(H, FH, W, FW, pad, stride, IC, FN)\n",
        "\n",
        "print('Output size(FN × OH × OW) : {} × {} × {}'.format(FN, out_h, out_w))\n",
        "print('Activation Size : {}'.format(activation_size))\n",
        "print('Number of parameters : {}'.format(params))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YshplTayl6n",
        "outputId": "4b89a764-9803-458f-caae-74e4346234fe"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output size(FN × OH × OW) : 20 × 9 × 9\n",
            "Activation Size : 1620\n",
            "Number of parameters : 1820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [Problème 11] (Problème avancé) Enquête sur la taille du filtre\n"
      ],
      "metadata": {
        "id": "Qzo9s_EByqgT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'une des raisons de préférer les noyaux de petite taille aux réseaux entièrement connectés est qu'ils réduisent les coûts de calcul et le partage des poids, ce qui conduit finalement à des poids moindres pour la rétropropagation. les convolutions 3x3 en une série de couches de convolution unidimensionnelles. Et cela s'est avéré être très rentable ! 3x3 est le choix optimal. Les filtres de convolution 3x3 fonctionnent en général, et c'est souvent le choix populaire !\n",
        "\n",
        "Les convolutions 1 x 1 ont été proposées dans l'article Network-in-network. Elles ont ensuite été très utilisées dans l'article Google Inception. Voici quelques avantages des convolutions 1 x 1 :\n",
        "\n",
        "Réduction de la dimensionnalité pour des calculs efficaces\n",
        "Intégration efficace à faible dimension, ou regroupement de caractéristiques.\n",
        "Réapplication de la non-linéarité après la convolution."
      ],
      "metadata": {
        "id": "JLGHVHQ0y6Rv"
      }
    }
  ]
}