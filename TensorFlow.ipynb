{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMIN+ayKKBXBQu1LeTs1q6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Baroka-wp/dive_ML/blob/master/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partie d'importation de bibliothèque\n"
      ],
      "metadata": {
        "id": "4BzEupceSZsU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kPd31V0gRw5_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from keras.datasets import mnist\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name() \n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "from tensorflow.python.keras.layers import  Input, Embedding, Dot, Reshape, Dense\n",
        "from tensorflow.python.keras.models import Model\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 1] Retour sur zéro"
      ],
      "metadata": {
        "id": "vdd4K0Ejhbrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- J'ai dû initialiser les poids\n",
        "- Le calcul matriciel de la propagation vers l'avant était nécessaire\n",
        "- Le calcul de la matrice de rétropropagation était nécessaire\n",
        "- J'avais besoin d'une boucle d'époque\n",
        "Calcul des pertes nécessaire"
      ],
      "metadata": {
        "id": "ADLNq02yhdn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Préparation du jeu de données\n"
      ],
      "metadata": {
        "id": "jDwZbWfzjC7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_init = load_iris()\n",
        "df = pd.DataFrame(df_init.data,columns=df_init.feature_names)\n",
        "target_df = pd.DataFrame(df_init.target,columns=[\"species\"])\n",
        "\n",
        "df = pd.concat([df,target_df], axis=1)\n",
        "df = df.loc[(df.species==1) | (df.species==2)]\n",
        "X = X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = df['species']\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y[y == 1] = 0\n",
        "y[y == 2] = 1\n",
        "\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "7v6_JLEoYXVI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 2] Considérez la correspondance entre scratch et TensorFlow\n"
      ],
      "metadata": {
        "id": "zZBXv2d-jKmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Création d'une classe pour le traitement par mini-batch\n"
      ],
      "metadata": {
        "id": "XZ-HF7qcetar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SampleIterator():\n",
        "    def __init__(self):\n",
        "        self.X = [1,2,3,4,5]\n",
        "        self.counter = 0\n",
        "        self.stop = len(self.X)\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self.counter >= self.stop:\n",
        "            raise StopIteration()\n",
        "        x = self.X[self.counter]\n",
        "        self.counter += 1\n",
        "        return x\n",
        "sample_iter = SampleIterator()\n",
        "for x in sample_iter:\n",
        "    print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXR3jSIKZI82",
        "outputId": "902b5189-c7a2-4e1b-d82f-79a9ce50848f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "metadata": {
        "id": "w4AAPhhcZPWN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Paramètres d'hyperparamètres\n"
      ],
      "metadata": {
        "id": "OgdD7VkxekMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1"
      ],
      "metadata": {
        "id": "veLCpURFef7b"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Déterminer la forme des entrées et des sorties du réseau\n"
      ],
      "metadata": {
        "id": "LMDHfEYeezW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])"
      ],
      "metadata": {
        "id": "rMWo4fdye0bm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQ02m3QHe3B1",
        "outputId": "0f9c4979-1bb6-4aad-c9fd-75ac6cfb79f3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # 重みとバイアスの宣言\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# ネットワーク構造の読み込み                               \n",
        "logits = example_net(X)\n",
        "\n",
        "# 目的関数\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 推定結果\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# 指標値計算\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# variableの初期化\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# 計算グラフの実行\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # エポックごとにループ\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # ミニバッチごとにループ\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hX4uZHY4jd27",
        "outputId": "5f507816-3e4f-4847-f996-f87a81f8a497"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7681, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3725, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1453, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4516, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4015, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1098, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0862, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1731, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2714, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3122, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1524, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2300, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1061, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1768, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0710, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0693, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "D'après ce qui précède, nous voyons que l'initialisation des poids est effectuée par tf.Variable(tf.random_normal) ; Adam est utilisé comme optimiseur et la fonction d'activation passe par tf.nn.relu."
      ],
      "metadata": {
        "id": "8_j93VXCjzp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 3] Créer un modèle d'Iris en utilisant les trois types de variables objectives\n"
      ],
      "metadata": {
        "id": "Hmy2dcvZj3al"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_init = load_iris()\n",
        "df = pd.DataFrame(df_init.data,columns=df_init.feature_names)\n",
        "target_df = pd.DataFrame(df_init.target,columns=[\"species\"])\n",
        "\n",
        "df = pd.concat([df,target_df], axis=1)\n",
        "X = X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = df['species']\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)"
      ],
      "metadata": {
        "id": "wiZWJtXvjdrr"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                             \n",
        "logits = example_net(X)\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jz9elYfQkwga",
        "outputId": "7b2355ad-35da-4b1b-be98-3944813b381a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 9.7833, val_loss : 10.6563, acc : 0.667, val_acc : 0.778\n",
            "Epoch 1, loss : 5.9695, val_loss : 3.0728, acc : 0.778, val_acc : 0.889\n",
            "Epoch 2, loss : 2.1762, val_loss : 1.8169, acc : 0.778, val_acc : 0.875\n",
            "Epoch 3, loss : 0.0070, val_loss : 0.6317, acc : 1.000, val_acc : 0.917\n",
            "Epoch 4, loss : 0.0056, val_loss : 0.5414, acc : 1.000, val_acc : 0.931\n",
            "Epoch 5, loss : 0.0002, val_loss : 0.5034, acc : 1.000, val_acc : 0.917\n",
            "Epoch 6, loss : 0.0360, val_loss : 0.8326, acc : 1.000, val_acc : 0.903\n",
            "Epoch 7, loss : 0.0045, val_loss : 0.9275, acc : 1.000, val_acc : 0.917\n",
            "Epoch 8, loss : 0.0030, val_loss : 0.8121, acc : 1.000, val_acc : 0.903\n",
            "Epoch 9, loss : 0.1338, val_loss : 1.0712, acc : 0.944, val_acc : 0.917\n",
            "test_acc : 0.967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 4] Créer un modèle de prix des logements\n"
      ],
      "metadata": {
        "id": "numKW9S3lTgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "dataset_path =\"/content/train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "y_pred = logits\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        loss_list.append(loss)\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    print(\"test_mse : {:.3f}\".format(loss))\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "irgNwMBPlU48",
        "outputId": "643a5ee5-c1e4-4ef1-8d29-1de370a703a2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 4964973.5000, val_loss : 3729021.5000\n",
            "Epoch 1, loss : 105874.4297, val_loss : 61050.7344\n",
            "Epoch 2, loss : 48980.4844, val_loss : 49638.3750\n",
            "Epoch 3, loss : 33488.4766, val_loss : 32951.8906\n",
            "Epoch 4, loss : 27598.9863, val_loss : 25192.6465\n",
            "Epoch 5, loss : 24195.6641, val_loss : 21238.5156\n",
            "Epoch 6, loss : 21619.9473, val_loss : 18860.6230\n",
            "Epoch 7, loss : 20570.0215, val_loss : 17997.9277\n",
            "Epoch 8, loss : 18731.9805, val_loss : 15998.9805\n",
            "Epoch 9, loss : 19999.5156, val_loss : 17339.9707\n",
            "test_mse : 19999.516\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Problème 5] Créer un modèle de MNIST\n"
      ],
      "metadata": {
        "id": "nhnbBo4qpRB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
        "\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10\n",
        "\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] \n",
        "    return layer_output\n",
        "                               \n",
        "logits = example_net(X)\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "max_Y = (tf.argmax(Y, 1))\n",
        "max_Y_pred = tf.argmax(logits, 1)\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "            total_acc += acc\n",
        "        total_loss /= n_samples\n",
        "        total_acc /= n_samples\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0thgO11pSZZ",
        "outputId": "b31cd72d-2734-4e47-c5a7-22dff870ddad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:93: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 0.9809, val_loss : 1.2141, acc : 0.500, val_acc : 0.648\n",
            "Epoch 1, loss : 0.9254, val_loss : 0.9614, acc : 0.600, val_acc : 0.776\n",
            "Epoch 2, loss : 0.7081, val_loss : 0.7212, acc : 0.700, val_acc : 0.845\n",
            "Epoch 3, loss : 0.6233, val_loss : 0.4046, acc : 0.700, val_acc : 0.903\n",
            "Epoch 4, loss : 0.6234, val_loss : 0.4170, acc : 0.800, val_acc : 0.896\n",
            "Epoch 5, loss : 0.6157, val_loss : 0.3721, acc : 0.800, val_acc : 0.916\n",
            "Epoch 6, loss : 0.7452, val_loss : 0.3962, acc : 0.700, val_acc : 0.917\n",
            "Epoch 7, loss : 0.6560, val_loss : 0.3623, acc : 0.800, val_acc : 0.924\n",
            "Epoch 8, loss : 0.4046, val_loss : 0.3142, acc : 0.900, val_acc : 0.927\n",
            "Epoch 9, loss : 0.2602, val_loss : 0.3467, acc : 0.900, val_acc : 0.932\n",
            "test_acc : 0.934\n"
          ]
        }
      ]
    }
  ]
}